# opendomains

**A scalable, automated system to archive web structures across all valid TLDs.**

---

## ğŸ§­ Project Overview

**opendomains** is a fully automated, GitHub Actions-friendly crawler that catalogs websites and their subpages into structured markdown documents. Designed for archival and research purposes, it navigates the modern web by:

- Generating a folder structure from ICANNâ€™s official `tlds-alpha-by-domain.txt`.
- Launching URL crawls defined in `config/init-url.txt`.
- **Respecting each website's `robots.txt` rules and skipping URLs that are disallowed for crawling.**
- Recursively discovering and saving reachable subpages into `domains/` as `.md` files.
- Maintaining a central CSV file in the `data/` directory for status tracking and concurrency control.

### ğŸ” Problem Statement

The web changes rapidly. Link rot, content drift, and lack of long-term availability make research and archival difficult. This project attempts to mitigate that by capturing live website structures in a human-readable, version-controllable format.

### ğŸ¯ Target Audience

- Researchers, digital preservationists, and web historians.
- Developers or AI agents needing large-scale web structure data.
- Open-source archivists aiming to replicate or extend Common Crawl-like behavior in minimal form.

---

## ğŸ—ï¸ Architecture & Design Principles

### ğŸ”§ Technology Stack

- **Language:** Python 3.10+
- **Execution Platform:** GitHub Actions
- **Data Storage:** Flat files (`.md`, `.json`) under the `domains/` directory and CSV files in the `data/` directory
- **Concurrency:** Python `threading` for multi-threaded crawling
- **Scheduler:** GitHub Actions cron jobs
- **Parsing:** `requests`, `beautifulsoup4`, `tldextract`

### âš™ï¸ Key Design Decisions

- **Concurrency & Locking:** Multi-threaded crawling is supported. Files are locked by checking editing status in the CSV file before write access.
- **Scalability:** Modular and parallelizable per TLD folder.
- **Fail-Safe Indexing:** CSV files in the `data/` directory track progress percentage, last update timestamps, and scraping state (`waiting`, `editing`, `finished`).
- **Non-intrusive GitHub Actions:** Crawls are divided and throttled to avoid timeouts and GitHub rate limits.

---

## ğŸ› ï¸ Installation & Setup

### ğŸ“¦ Dependencies

Ensure the following Python packages are installed:

```bash
pip install -r requirements.txt
```

> If `requirements.txt` is missing, the main dependencies are:
>
> - `requests`
> - `beautifulsoup4`
> - `tldextract`
> - `markdownify`
> - `python-dotenv` (if using environment variables)

### ğŸ§ª Environment Variables (Optional)

You can configure settings in a `.env` file:

```env
THREADS=5
TIMEOUT=10
RETRY_LIMIT=3
```

### ğŸ—‚ï¸ Setup Instructions

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/opendomains.git
   cd opendomains
   ```

2. Populate the initial crawl configuration:

   - Download `tlds-alpha-by-domain.txt` into `config/`.
   - Add seed URLs to `config/init-url.txt`.

3. Generate initial folder structure:

   ```bash
   python scripts/init_folders.py
   ```

4. Start the crawler (locally):

   ```bash
   python scripts/crawl.py
   ```

---

## ğŸš€ Usage Guidelines

### ğŸ§­ Running Locally

```bash
python scripts/crawl.py
```

This will:

- Read `init-url.txt`.
- Check and respect the CSV file lock in the `data/` directory.
- **Check and respect each site's `robots.txt` before crawling any URL.**
- Begin crawling each seed and saving subpages under their respective TLD folders.

### ğŸ› GitHub Actions Routine

The project includes `.github/workflows/crawl.yml` which:

- Runs scheduled jobs using GitHub Actions.
- Limits concurrent execution to prevent timeout (staggered by repo or subfolder).
- Updates CSV files in the `data/` directory with crawl status and metadata.

### ğŸ” Workflow States

Each target domain in the CSV file will reflect:

- `waiting`: Not yet processed
- `editing`: Actively being crawled
- `finished`: Crawl completed
- `error`: Failed or skipped after retry

---

## ğŸ—ƒï¸ Code & Folder Structure

```
opendomains/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ init-url.txt          # Seed URLs to start crawling
â”‚   â””â”€â”€ tlds-alpha-by-domain.txt # Source of TLDs from ICANN
â”‚
â”œâ”€â”€ domains/
â”‚   â””â”€â”€ com/
â”‚       â””â”€â”€ example.com/
â”‚           â”œâ”€â”€ index.md
â”‚           â”œâ”€â”€ about.md
â”‚           â””â”€â”€ contact.md
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init_folders.py       # Bootstrap script for folder creation
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ crawl.yml             # Scheduled GitHub Actions workflow
â””â”€â”€ README.md

# Removed index.json and updated documentation to reflect CSV/data/ approach.
```

### ğŸ§¹ Naming & Formatting

- Folder names follow TLD and domain hierarchy.
- Markdown file names mirror page paths (`about`, `contact`, etc.).
- Code uses [PEP8](https://peps.python.org/pep-0008/) formatting.

---

## ğŸ¤ Contribution & Collaboration

### ğŸ”€ Branching Strategy

- `main`: Production branch (GitHub Actions runs here).
- `dev`: Development and new feature integration.
- Feature branches: `feature/<name>`

### ğŸ§ª Pull Requests

- Include clear descriptions.
- Reference related issues.
- Ensure tests or dry runs are included.

### ğŸ” Code Review Process

- All PRs require at least one review before merging.
- Review checklist includes structure, naming, and CSV file safety checks.

### ğŸ Reporting Issues & Suggestions

- Use GitHub Issues to report bugs or request features.
- Label clearly (`bug`, `feature`, `question`, etc.).

---

## ğŸªª Licensing & Contact Information

### ğŸ“„ License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE) for full details.
